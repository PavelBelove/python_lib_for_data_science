{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "micro - рассчитывает F1 на глобальном уровне, просто истинные срабатывания, ложные, и ложные негативы, вне зависимости от классов. Неплохо себя показывает на несбалансированных данных.\n",
    "macro - вычисление метрики для каждого класса и получение невзвешенного среднего. Честно говоря, примера где его применение было бы оправдано, в голову не приходит. Если данные не сбалансированы, критерий будет необъективен. Если же сбалансированы - даст практически тот же результат что и weighted.\n",
    "weighted - то же, что и макро, но с балансировкой по классам. То есть, среднее взвешенное, и \"влияние\" каждого класса на метрику, учитывает объем данных этого класса в выборке.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все три алгоритма - современные реализации бустинга, и в целом, выполняют одни и те же задачи. в \"общем зачете\" на сегодня чуть лидирует catboost, с оговоркой что все зависит от конкретных данных и конкретной задачи. Однако, как эти алгоритмы в настоящий момент развиваются, так и разрабатываются новые. Поэтому при решении конкретной задачи стоит протестировать все алгоритмы, и то не факт, что лучший в тестах окажется действительно лучшим, поскольку многое зависит от гиперпараметров и подготовки данных, вполне возможна ситуация, что при тонкой настройке лучше \"отстреляется\" другой алгоритм. Поэтому, при наличии достаточного кол-ва ресурсов, лучше обучить разные, потенциально применимые модели, и уже по итоговому результату выбрать наилучший результат.\n",
    "\n",
    "\n",
    "#### xgboost (extreme gradient boosting)\n",
    "это модернизированный градиентный бустинг, с идеальной комбинацией оптимизации ПО и железа для получения точных релузьтатов за короткое время с минимальным использованием вычислительных ресурсов. Под капотом ансамбли методов деревьев, которые используют принцип бустинга слабых учеников.\n",
    "\n",
    "#### lightgbm\n",
    "относится к классу ансамблевых алгоритмов машинного обучения, которые могут использоваться для задач классификации или регрессионного прогностического моделирования.\n",
    "Реализация вводит две ключевые идеи: GOSS и EFB.\n",
    "\n",
    "Градиентная односторонняя выборка (GOSS) является модификацией градиентного бустинга, который фокусирует внимание на тех учебных примерах, которые приводят к большему градиенту, в свою очередь, ускоряя обучение и уменьшая вычислительную сложность метода.\n",
    "С помощью GOSS исключается значительная доля экземпляров данных с небольшими градиентами и используется только остальные экземпляры для оценки прироста информации. Мы доказываем, что, поскольку экземпляры данных с большими градиентами играют более важную роль в вычислении информационного выигрыша, GOSS может получить довольно точную оценку информационного выигрыша с гораздо меньшим размером данных.\n",
    "Exclusive Feature Bundling (объединение взаимоисключающих признаков), или EFB, — это подход объединения разрежённых (в основном нулевых) взаимоисключающих признаков, таких как категориальные переменные входных данных, закодированные унитарным кодированием. Таким образом, это тип автоматического подбора признаков.\n",
    "Преимущества перед XGBoost\n",
    "использует алгоритм на основе гистограммы, то есть он объединяет непрерывные значения признаков в дискретные ячейки, которые ускоряют процедуру обучения.\n",
    "заменяет непрерывные значения на дискретные ячейки, что приводит к меньшему использованию памяти.\n",
    "он создает гораздо более сложные деревья, следуя подходу разделения листьев, а не поуровневому подходу, который является основным фактором в достижении более высокой точности. Однако иногда это может привести к переобучению, чего можно избежать, установив параметр max_depth.\n",
    "одинаково хорошо работает с большими наборами данных при значительном сокращении времени обучения по сравнению с XGBOOST.\n",
    "\n",
    "#### catboost\n",
    "библиотека градиентного бустинга, созданная Яндексом. Использует небрежные (oblivious) деревья решений, чтобы вырастить сбалансированное дерево. Одни и те же функции используются для создания левых и правых разделений (split) на каждом уровне дерева. По сравнению с классическими деревьями, небрежные деревья более эффективны при реализации на процессоре и просты в обучении.\n",
    "Наиболее распространенными способами обработки категориальных данных в машинном обучении является one-hot кодирование и кодирование лейблов. CatBoost позволяет использовать категориальные признаки без необходимости их предварительно обрабатывать. При использовании CatBoost мы не должны пользоваться one-hot кодированием, поскольку это влияет на скорость обучения и на качество прогнозов. Вместо этого мы просто задаем категориальные признаки с помощью параметра cat_features. Преимущества использования CatBoost\n",
    "CatBoost позволяет проводить обучение на нескольких GPU.\n",
    "Библиотека позволяет получить отличные результаты с параметрами по умолчанию, что сокращает время, необходимое для настройки гиперпараметров.\n",
    "Обеспечивает повышенную точность за счет уменьшения переобучения.\n",
    "Возможность быстрого предсказания с применением модели CatBoost;\n",
    "Обученные модели CatBoost можно экспортировать в Core ML для вывода на устройстве (iOS).\n",
    "Умеет под капотом обрабатывать пропущенные значения.\n",
    "Может использоваться для регрессионных и классификационных задач.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
